{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55e7bed3-d471-4560-b5ce-4e3db9eafc0d",
   "metadata": {},
   "source": [
    "# U-Net Model for Segmentation\n",
    "\n",
    "We will convert this notebook into script since this is just the model itself, no data is attached. We need to compile and train our model and then record loss and metrics with our predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1f699c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf178e8e-92ed-4e18-8f0a-947282dd84db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def double_conv (x, c_out): #x is the input tensor, c_out is the number of output channels\n",
    "    x = layers.Conv2D(c, 3, padding=\"same\", use_bias =False)(x) # 3x3 2D convolution with equal output height/width\n",
    "    x = layers.BatchNormalization()(x) # Normalizes the feature maps so that each channel has a stable mean and variance\n",
    "    x = layers.ReLU()(x) # ReLU activation\n",
    "    x = layers.Conv2D(c_out, 3, padding=\"same\", use_bias=False)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a208903f-6558-4b82-8284-3adc6908bc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def UNetSmallTF(input_shape=(256,256,1), num_classes=3, base=32): # takes 256x256 greyscale images with 3 classes and a baseline of 32 filters\n",
    "    inputs = layers.Input(shape=input_shape) # input tensor\n",
    "\n",
    "\n",
    "    #Encoder - each stage downsamples via MaxPooling, reducing spacial resolution and extracting features\n",
    "    \n",
    "    c1 = double_conv(inputs, base)\n",
    "    p1 = layers.MaxPool2D(2)(c1) # 128 x 128 x 32, MaxPooling reduces resolution by 2x\n",
    "\n",
    "    c2 = double_conv(p1, base*2) # 64 filters\n",
    "    p2 = layers.MaxPool2D(2)(c2) # 64 x 64 x 64\n",
    "\n",
    "    c3 = double_conv(p2, base*4) # 128 filters\n",
    "    p3 = layers.MaxPool2D(2)(c3) # 32 x 32 x 128\n",
    "\n",
    "    # Bottleneck\n",
    "    bn = double_conv(p3, base*8) # stops at 256 filters since it's at the bottom of the U\n",
    "\n",
    "    # Decoder - upsampling the features back into the input resolution\n",
    "    \n",
    "    u3 = layers.Conv2DTranspose(base*4, 2, strides=2, padding=\"same\")(bn) # learnable upsampling, double spacial size 32x32 -> 64x64\n",
    "    u3 = layers.Concatenate()([u3, c3]) # concatenate with c3 to reintroduce spatial features using, c3 is a \"skip connection\"\n",
    "    u3 = double_conv(u3, base*4) \n",
    "\n",
    "    u2 = layers.Conv2DTranspose(base*2, 2, strides=2, padding=\"same\")(u3)\n",
    "    u2 = layers.Concatenate()([u2, c2])\n",
    "    u2 = double_conv(u2, base*2)\n",
    "\n",
    "    u1 = layers.Conv2DTranspose(base, 2, strides=2, padding=\"same\")(u2)\n",
    "    u1 = layers.Concatenate()([u1, c1])\n",
    "    u1 = double_conv(u1, base)\n",
    "\n",
    "    logits = layers.Conv2D(num_classes, 1, padding=\"same\")(u1)  # a 1x1 convolution acts as a linear classifier at each pixel\n",
    "    return Model(inputs, logits, name=\"UNetSmallTF\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
